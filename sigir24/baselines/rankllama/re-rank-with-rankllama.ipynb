{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RankLLaMA\n",
    "\n",
    "This jupyter notebook uses [rankllama](https://huggingface.co/castorini/rankllama-v1-7b-lora-passage/) to re-rank documents in the context [ReNeuIR 2024](https://reneuir.org/). Please look at the [corresponding publications](https://arxiv.org/pdf/2310.08319) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from tira.third_party_integrations import ir_datasets, persist_and_normalize_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load and Transform the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"from tira.third_party_integrations import ir_datasets\" import patches \"ir_datasets.load\"\n",
    "# so that it loads the dataset injected into the tira sandbox when executed within the sandbox.\n",
    "# I.e., we only ensure that it runs on a minimal spot-check dataset here.\n",
    "dataset = ir_datasets.load('reneuir-2024/re-rank-spot-check-20240624-training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_to_rerank_data = {}\n",
    "\n",
    "for i in dataset.scoreddocs_iter():\n",
    "    if i.query_id not in qid_to_rerank_data:\n",
    "        qid_to_rerank_data[i.query_id] = {'query': i.query.default_text(), 'search_results': []}\n",
    "    \n",
    "    qid_to_rerank_data[i.query_id]['search_results']  += [\n",
    "        {'docid': i.doc_id, 'text': i.document.default_text(), 'score': i.score}\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Re-Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForSequenceClassification is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66052ea357a45e799b876a76ac604be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranker is LlamaForSequenceClassification(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaFlashAttention2(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (score): Linear(in_features=4096, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Will use device:', device)\n",
    "\n",
    "def get_model(peft_model_name):\n",
    "    config = PeftConfig.from_pretrained(peft_model_name)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path, num_labels=1, attn_implementation=\"flash_attention_2\")\n",
    "    model = PeftModel.from_pretrained(base_model, peft_model_name)\n",
    "    model = model.merge_and_unload()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = get_model('castorini/rankllama-v1-7b-lora-passage')\n",
    "model.to(device)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print('Ranker is', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:06<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "run = []\n",
    "for qid in tqdm(qid_to_rerank_data):\n",
    "    query = qid_to_rerank_data[qid]['query']\n",
    "    search_results = qid_to_rerank_data[qid]['search_results']\n",
    "    rerank_unsorted_results = []\n",
    "    input_texts = [f'query: {query} document: {doc[\"text\"]}' for doc in search_results]\n",
    "    for i in range(0, len(input_texts), batch_size):\n",
    "        batch = input_texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs.to(device))\n",
    "        scores = outputs.logits.squeeze().cpu().tolist()\n",
    "        rerank_unsorted_results += [{'docid': search_results[i]['docid'], 'score': scores[i]} for i in range(len(scores))]\n",
    "    rerank_sorted_results = sorted(rerank_unsorted_results, key=lambda x: x['score'], reverse=True)\n",
    "    # add qid score and docid to run\n",
    "    for result in rerank_sorted_results:\n",
    "        run += [{\"qid\": qid, \"score\": result['score'], \"docno\": result['docid']}]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Persist run file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \".\".\n",
      "Done. run file is stored under \"./run.txt\".\n"
     ]
    }
   ],
   "source": [
    "run = pd.DataFrame(run)\n",
    "\n",
    "persist_and_normalize_run(run, system_name=f'rankllama', default_output='.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
