{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RankLLaMA\n",
    "\n",
    "This jupyter notebook uses [rankllama](https://huggingface.co/castorini/rankllama-v1-7b-lora-passage/) to re-rank documents in the context [ReNeuIR 2024](https://reneuir.org/). Please look at the [corresponding publications](https://arxiv.org/pdf/2310.08319) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from tira.third_party_integrations import ir_datasets, persist_and_normalize_run\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load and Transform the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"from tira.third_party_integrations import ir_datasets\" import patches \"ir_datasets.load\"\n",
    "# so that it loads the dataset injected into the tira sandbox when executed within the sandbox.\n",
    "# I.e., we only ensure that it runs on a minimal spot-check dataset here.\n",
    "dataset = ir_datasets.load('reneuir-2024/re-rank-spot-check-20240624-training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_to_rerank_data = {}\n",
    "\n",
    "for i in dataset.scoreddocs_iter():\n",
    "    if i.query_id not in qid_to_rerank_data:\n",
    "        qid_to_rerank_data[i.query_id] = {'query': i.query.default_text(), 'search_results': []}\n",
    "    \n",
    "    qid_to_rerank_data[i.query_id]['search_results']  += [\n",
    "        {'docid': i.doc_id, 'text': i.document.default_text(), 'score': i.score}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Re-Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use device: cuda\n",
      "Use \"/root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9\" as tokenizer.\n",
      "Use \"/root/.cache/huggingface/hub/models--castorini--rankllama-v1-7b-lora-passage/snapshots/a079eadc8530520a8259e694212fd201531753a9\" as model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cce68a06667406199ffd75ac3073e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranker is LlamaForSequenceClassification(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaFlashAttention2(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (score): Linear(in_features=4096, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Will use device:', device)\n",
    "\n",
    "def get_model(peft_model_name):\n",
    "    config = PeftConfig.from_pretrained(peft_model_name)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path, num_labels=1, attn_implementation=\"flash_attention_2\")\n",
    "    model = PeftModel.from_pretrained(base_model, peft_model_name)\n",
    "    model = model.merge_and_unload()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer_path = snapshot_download('meta-llama/Llama-2-7b-hf', local_files_only=True)\n",
    "print(f'Use \"{tokenizer_path}\" as tokenizer.')\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "model_path = snapshot_download('castorini/rankllama-v1-7b-lora-passage', local_files_only=True)\n",
    "print(f'Use \"{model_path}\" as model.')\n",
    "model = get_model(model_path)\n",
    "model.to(device)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print('Ranker is', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                       | 0/5 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.00s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "run = []\n",
    "for qid in tqdm(qid_to_rerank_data):\n",
    "    query = qid_to_rerank_data[qid]['query']\n",
    "    search_results = qid_to_rerank_data[qid]['search_results']\n",
    "    rerank_unsorted_results = []\n",
    "    input_texts = [f'query: {query} document: {doc[\"text\"]}' for doc in search_results]\n",
    "    for i in range(0, len(input_texts), batch_size):\n",
    "        batch = input_texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs.to(device))\n",
    "        scores = outputs.logits.squeeze().cpu().tolist()\n",
    "        if len(batch) == 1:\n",
    "            # if we only have one input element in the batch, scores contains the float, not a list of floats.\n",
    "            scores = [scores]\n",
    "        \n",
    "        rerank_unsorted_results += [{'docid': search_results[i]['docid'], 'score': scores[i]} for i in range(len(scores))]\n",
    "    rerank_sorted_results = sorted(rerank_unsorted_results, key=lambda x: x['score'], reverse=True)\n",
    "    # add qid score and docid to run\n",
    "    for result in rerank_sorted_results:\n",
    "        run += [{\"qid\": qid, \"score\": result['score'], \"docno\": result['docid']}]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Persist run file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The run file is normalized outside the TIRA sandbox, I will store it at \".\".\n",
      "Done. run file is stored under \"./run.txt\".\n"
     ]
    }
   ],
   "source": [
    "run = pd.DataFrame(run)\n",
    "\n",
    "persist_and_normalize_run(run, system_name=f'rankllama', default_output='.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
